# HÆ°á»›ng dáº«n Config cho Dataset Nhá» (~8,000 tokens)

## ğŸ¯ Tá»•ng quan

Vá»›i dataset kÃ­ch thÆ°á»›c **~8,000 tokens** (training ~7,200, validation ~800), cáº§n tá»‘i Æ°u hÃ³a Ä‘á»ƒ:
1. **TrÃ¡nh overfitting** - MÃ´ hÃ¬nh há»c quÃ¡ ká»¹ training data
2. **TÄƒng generalization** - MÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t trÃªn dá»¯ liá»‡u má»›i
3. **Tá»‘i Æ°u tá»‘c Ä‘á»™** - Training nhanh vá»›i dataset nhá»

---

## ğŸ“Š So sÃ¡nh Config: Máº·c Ä‘á»‹nh vs Tá»‘i Æ°u

### **Neural Network (MLP)**

| Tham sá»‘ | Máº·c Ä‘á»‹nh (Large Data) | Tá»‘i Æ°u (8K Data) | LÃ½ do |
|---------|----------------------|------------------|-------|
| `NN_EPOCHS` | 50 | **100** | Dataset nhá», cÃ³ thá»ƒ train lÃ¢u hÆ¡n |
| `NN_BATCH_SIZE` | 128 | **64** | Batch nhá» â†’ nhiá»u updates â†’ há»c tá»‘t hÆ¡n |
| `NN_HIDDEN_DIMS` | 512,256,128 | **256,128** | MÃ´ hÃ¬nh Ä‘Æ¡n giáº£n hÆ¡n â†’ Ã­t overfit |
| `NN_DROPOUT` | 0.3 | **0.4** | Dropout cao hÆ¡n â†’ regularization máº¡nh |

**Giáº£i thÃ­ch chi tiáº¿t:**
- âœ… **Batch size 64**: Vá»›i 7,200 samples, má»—i epoch cÃ³ ~112 updates (vs 56 updates vá»›i batch 128)
- âœ… **Hidden dims 256,128**: Giáº£m tá»« ~500K parameters xuá»‘ng ~200K parameters
- âœ… **Dropout 0.4**: Táº¯t ngáº«u nhiÃªn 40% neurons â†’ buá»™c mÃ´ hÃ¬nh há»c robust features

---

### **Random Forest**

| Tham sá»‘ | Máº·c Ä‘á»‹nh (Large Data) | Tá»‘i Æ°u (8K Data) | LÃ½ do |
|---------|----------------------|------------------|-------|
| `RF_N_ESTIMATORS` | 200 | **150** | Ãt trees â†’ nhanh hÆ¡n, Ã­t overfit |
| `RF_MAX_DEPTH` | None (unlimited) | **15** | Giá»›i háº¡n Ä‘á»™ sÃ¢u â†’ trÃ¡nh há»c noise |
| `RF_MIN_SAMPLES_SPLIT` | 5 | **10** | Cáº§n nhiá»u samples hÆ¡n Ä‘á»ƒ split |
| `RF_MIN_SAMPLES_LEAF` | 2 | **4** | Leaf pháº£i cÃ³ Ã­t nháº¥t 4 samples |

**Giáº£i thÃ­ch chi tiáº¿t:**
- âœ… **Max depth 15**: Vá»›i 7,200 samples, depth 15 Ä‘á»§ Ä‘á»ƒ há»c patterns mÃ  khÃ´ng overfit
- âœ… **Min samples split 10**: Chá»‰ split náº¿u node cÃ³ â‰¥10 samples (0.14% cá»§a data)
- âœ… **Min samples leaf 4**: Má»—i leaf pháº£i Ä‘áº¡i diá»‡n cho Ã­t nháº¥t 4 samples

---

### **XGBoost** (Khuyáº¿n nghá»‹ cao nháº¥t)

| Tham sá»‘ | Máº·c Ä‘á»‹nh (Large Data) | Tá»‘i Æ°u (8K Data) | LÃ½ do |
|---------|----------------------|------------------|-------|
| `XGB_N_ESTIMATORS` | 200 | **150** | Ãt trees vá»›i early stopping |
| `XGB_MAX_DEPTH` | 6 | **4** | Trees nÃ´ng hÆ¡n â†’ Ã­t overfit |
| `XGB_LEARNING_RATE` | 0.1 | **0.05** | Há»c cháº­m hÆ¡n â†’ generalize tá»‘t |
| `XGB_SUBSAMPLE` | 0.8 | **0.7** | DÃ¹ng 70% data má»—i tree |
| `XGB_COLSAMPLE_BYTREE` | 0.8 | **0.7** | DÃ¹ng 70% features má»—i tree |
| `XGB_GAMMA` | 0 | **0.1** | YÃªu cáº§u loss giáº£m â‰¥0.1 Ä‘á»ƒ split |
| `XGB_REG_ALPHA` | 0 | **0.1** | L1 regularization |
| `XGB_REG_LAMBDA` | 1 | **2** | L2 regularization máº¡nh hÆ¡n |
| `XGB_EARLY_STOPPING` | 10 | **15** | KiÃªn nháº«n hÆ¡n trÆ°á»›c khi dá»«ng |

**Giáº£i thÃ­ch chi tiáº¿t:**
- âœ… **Max depth 4**: Trees nÃ´ng â†’ má»—i tree há»c simple patterns â†’ ensemble máº¡nh
- âœ… **Learning rate 0.05**: Má»—i tree Ä‘Ã³ng gÃ³p Ã­t hÆ¡n â†’ cáº§n nhiá»u trees â†’ robust hÆ¡n
- âœ… **Subsample 0.7**: Má»—i tree chá»‰ tháº¥y 70% data â†’ giá»‘ng bagging â†’ trÃ¡nh overfit
- âœ… **Gamma 0.1**: Chá»‰ split náº¿u loss giáº£m Ä‘Ã¡ng ká»ƒ â†’ trÃ¡nh splits khÃ´ng cáº§n thiáº¿t
- âœ… **Reg_lambda 2**: Penalty máº¡nh cho weights lá»›n â†’ smooth predictions

---

## ğŸ“ NguyÃªn táº¯c chung cho Small Dataset

### **1. Giáº£m Model Complexity**
```
LÃ½ do: MÃ´ hÃ¬nh phá»©c táº¡p dá»… há»c thuá»™c lÃ²ng training data
CÃ¡ch lÃ m:
  - Neural Network: Ãt layers, Ã­t neurons
  - Random Forest: Giá»›i háº¡n depth, tÄƒng min_samples
  - XGBoost: Shallow trees, strong regularization
```

### **2. TÄƒng Regularization**
```
LÃ½ do: NgÄƒn mÃ´ hÃ¬nh fit quÃ¡ sÃ¡t vá»›i training data
CÃ¡ch lÃ m:
  - Neural Network: Dropout cao (0.4-0.5)
  - Random Forest: Min_samples_split/leaf cao
  - XGBoost: L1/L2 regularization, gamma
```

### **3. Giáº£m Batch Size (Neural Network)**
```
LÃ½ do: Nhiá»u updates hÆ¡n má»—i epoch â†’ há»c tá»‘t hÆ¡n
CÃ´ng thá»©c: batch_size â‰ˆ sqrt(training_size)
  - 7,200 samples â†’ batch ~64-85
```

### **4. TÄƒng Training Time**
```
LÃ½ do: Dataset nhá» â†’ má»—i epoch nhanh â†’ cÃ³ thá»ƒ train lÃ¢u
CÃ¡ch lÃ m:
  - Neural Network: TÄƒng epochs (50 â†’ 100)
  - XGBoost: TÄƒng early_stopping_rounds (10 â†’ 15)
```

---

## ğŸ“ˆ Ká»³ vá»ng Accuracy vá»›i Config Tá»‘i Æ°u

### **Baseline (Logistic Regression)**
- Top 1: **62.23%** âœ… (Ä‘Ã£ cháº¡y)
- Top 5: **92.66%** âœ… (Ä‘Ã£ cháº¡y)

### **Vá»›i Config Tá»‘i Æ°u:**

| PhÆ°Æ¡ng phÃ¡p | Top 1 (dá»± kiáº¿n) | Top 5 (dá»± kiáº¿n) | Cáº£i thiá»‡n |
|-------------|-----------------|-----------------|-----------|
| **Neural Network** | 68-73% | 94-96% | +6-11% |
| **Random Forest** | 70-76% | 95-97% | +8-14% |
| **XGBoost** | 72-78% | 96-98% | +10-16% ğŸ† |

---

## âš ï¸ Dáº¥u hiá»‡u Overfitting cáº§n chÃº Ã½

### **Khi cháº¡y training, náº¿u tháº¥y:**

1. **Training accuracy >> Validation accuracy**
   ```
   VÃ­ dá»¥: Train 95%, Validation 65%
   â†’ Overfitting nghiÃªm trá»ng!
   ```
   **Giáº£i phÃ¡p:**
   - TÄƒng dropout (NN)
   - Giáº£m max_depth (RF, XGB)
   - TÄƒng regularization (XGB)

2. **Validation accuracy giáº£m sau vÃ i epochs**
   ```
   Epoch 20: Val 70%
   Epoch 30: Val 72%
   Epoch 40: Val 69% â† Báº¯t Ä‘áº§u overfit
   ```
   **Giáº£i phÃ¡p:**
   - Early stopping sáº½ tá»± Ä‘á»™ng dá»«ng
   - Giáº£m sá»‘ epochs/estimators

3. **Perfect training accuracy (100%)**
   ```
   â†’ MÃ´ hÃ¬nh há»c thuá»™c lÃ²ng data!
   ```
   **Giáº£i phÃ¡p:**
   - TÄƒng regularization máº¡nh hÆ¡n
   - Giáº£m model complexity

---

## ğŸš€ CÃ¡ch cháº¡y vá»›i Config má»›i

### **BÆ°á»›c 1: Reload config**
```bash
# Config Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t tá»± Ä‘á»™ng
source config.env
```

### **BÆ°á»›c 2: Cháº¡y tá»«ng phÆ°Æ¡ng phÃ¡p**

**XGBoost (Khuyáº¿n nghá»‹ cao nháº¥t):**
```bash
bash scripts/train_set/classifier_mapping/xgboost_classifier.sh
```

**Neural Network:**
```bash
bash scripts/train_set/classifier_mapping/neural_network.sh
```

**Random Forest:**
```bash
bash scripts/train_set/classifier_mapping/random_forest.sh
```

### **BÆ°á»›c 3: Xem káº¿t quáº£**
```bash
# Thay Ä‘á»•i fileDir trong get_prediction_stat.sh cho tá»«ng method:
# result_xgb, result_nn, result_rf

bash scripts/train_set/classifier_mapping/get_prediction_stat.sh
```

---

## ğŸ’¡ Tips NÃ¢ng cao

### **1. Grid Search cho XGBoost**
Náº¿u muá»‘n tÃ¬m config tá»‘t nháº¥t:
```bash
# Thá»­ cÃ¡c giÃ¡ trá»‹ khÃ¡c nhau:
XGB_MAX_DEPTH=3,4,5
XGB_LEARNING_RATE=0.03,0.05,0.1
XGB_REG_LAMBDA=1,2,3
```

### **2. Ensemble nhiá»u models**
```python
# Káº¿t há»£p predictions tá»« 3 models
final_prediction = vote(xgb_pred, rf_pred, nn_pred)
```

### **3. Data Augmentation**
Náº¿u cÃ³ thá»ƒ, tÄƒng data báº±ng cÃ¡ch:
- Synonym replacement
- Back-translation
- Paraphrasing

---

## ğŸ“Š Monitoring Training

### **Neural Network:**
```
Epoch [10/100], Loss: 2.1234, Accuracy: 65.23%
Epoch [20/100], Loss: 1.8765, Accuracy: 68.45%
...
```
- Loss giáº£m Ä‘á»u â†’ Tá»‘t âœ…
- Loss tÄƒng â†’ Overfitting âš ï¸

### **XGBoost:**
```
[0]     validation-mlogloss:2.1234
[10]    validation-mlogloss:1.8765
[20]    validation-mlogloss:1.7654
```
- mlogloss giáº£m Ä‘á»u â†’ Tá»‘t âœ…
- mlogloss tÄƒng â†’ Early stopping sáº½ dá»«ng âœ…

---

## ğŸ¯ Káº¿t luáº­n

Vá»›i dataset **~8,000 tokens**, config Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u Ä‘á»ƒ:
- âœ… **TrÃ¡nh overfitting** vá»›i regularization máº¡nh
- âœ… **TÄƒng generalization** vá»›i model Ä‘Æ¡n giáº£n hÆ¡n
- âœ… **Training nhanh** vá»›i Ã­t parameters hÆ¡n

**Khuyáº¿n nghá»‹:** Cháº¡y XGBoost trÆ°á»›c, sau Ä‘Ã³ so sÃ¡nh vá»›i Neural Network vÃ  Random Forest.

ChÃºc báº¡n Ä‘áº¡t accuracy cao! ğŸš€
